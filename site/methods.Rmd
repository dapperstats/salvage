---
title: "Salvage Data Methods"
date: "`r format(Sys.time(), '%B %d, %Y')`"
author: "Juniper L. Simonis"
---

Here, we describe the methods used to analyze the salvage database, both within an automated pipeline and locally. 

As developed below, the general methodology is depecited in the [`.travis.yml`](https://github.com/dapperstats/salvage/blob/master/.travis.yml) file, which shows full remote implementation.

<br>

# Software and Systems


To promote cross-platform availability and future reliability, we leverage a [software container approach](https://www.docker.com/resources/what-container) via [`Docker`](https://www.docker.com), which allows any user to establish stable runtime environments for data retrieval and calculation.
[A container is an instance of a software environment spun-up from an image that has been defined by a `Dockerfile`](https://docs.docker.com/engine/docker-overview/).

<br>

## Continuous Deployment

For continuous integration and analysis of newly posted data and continuous deployment of the data products and website, we follow the general approach of White et al. (2019). 
We host our code and data on [GitHub](https://github.com), use [Travis CI](https://travis-ci.org) to orchestrate compute, run analysis and related code in [`R`](https://www.r-project.org/), deploy our website via [Netlify](https://www.netlify.com/), and archive everything on [Zenodo](https://www.netlify.com/).
The data and output are updated daily via [`cron` jobs](https://docs.travis-ci.com/user/cron-jobs/) on [`travis-ci`](https://travis-ci.org/dapperstats/salvage) with a recipe (a.k.a. job lifecycle) described by the [`.travis.yml` file](https://github.com/dapperstats/salvage/blob/master/.travis.yml).

<br>

## Runtime Environments

<br>

### Docker

To use a container approach, we need to have `Docker` installed first. 

The standard [Travis CI](https://travis-ci.org) `bash` runtime environment comes pre-loaded with a small suite of programs, including `Docker`.
As such, we do not need to install it during our continuous integration workflow in order to use containers.

However, this is likely not to be the case for users establishing local versions of the pipeline.
As needed, then, [install `Docker`](https://docs.docker.com/get-docker/).
The specific instructions vary depending on your operating system, so pay special attention to that.

<br>

### Collecting Software Images

Following [best practices for Docker containers](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/), we have decoupled our overall workflow into two major components, each of which has its own image:
1. Data retrieval via the [`accessor` image](https://hub.docker.com/r/dapperstats/accessor)
2. Data summary/analysis/presentation via the [`salvage` image](https://hub.docker.com/r/dapperstats/salvage)

It is likely that in the future the second stage will be further decoupled into separate image (*e.g.*, "summary and analysis" and "presentation"), but that is not yet enacted.

Within a standard [Travis CI job](https://docs.travis-ci.com/user/job-lifecycle/), we use the [`bash` language](https://git.savannah.gnu.org/cgit/bash.git) and set the user with `sudo` privileges.

We then pull the remote images of the Docker containers from [Docker Hub](https://hub.docker.com/) to the Travis server:

```{bash, eval = FALSE}
docker pull dapperstats/accessor
docker pull dapperstats/salvage
```

<br>

### Instantiating Containers

At this point we can now `docker run` the images to produce containers in our runtime:

```{bash, eval = FALSE}
docker run --name acc --restart=always -it dapperstats/accessor
docker run --name salv --restart=always -itd dapperstats/salvage
```

We name the containers `acc` and `salv` for `accessor` and `salvage`, respectively, and keep them restarted so that their content is more quickly accessible.
The additional flags used are
 - `d` runs the container in the background (needed because there is no command line within `salvage`)
 - `i` keeps the container's `stdin` open
 - `t` allocates a pseudo-TTY to the `stdin`

<br>

## Populating Files

<br>

### Data Access

<br>

#### Retrieve the Salvage Database

The [`accessor` image](https://hub.docker.com/r/dapperstats/accessor), is defined with the software necessary to retrieve a remote Access<sup>&reg;</sup> database, convert it to a set of `.csv` files named by the tables in the database, and read the `.csv` files into [`R`](). 
The `accessor` image also contains `bash` scripts to perform the retrieval and conversion, which are executed at the creation of a container.
The default settings for the image are configured for the "current" (1993 - Present) salvage database, so the `acc` container will include the most up-to-date version of the salvage data, collected within the `data` folder. 
Code for the construction of the `accessor` image and details of its scripts are available in its [repository](https://www.github.com/dapperstats/accessor).

<br>

#### Share the Data 

The data that the `acc` container creates are located within itself and need to be shared to the build folder for archiving as well as to the `salv` container for analysis and presentation.
The main `Docker` command for moving files around is `cp`.
At the present, there is no capacity in `Docker` to copy between containers, so instead we copy the `data` folder out from `acc` and then make another copy into `salv`:

```{bash, eval = FALSE}
docker cp acc:/data .
docker cp data salv:/
```

Any existing files within the local `data` folder will be overwritten with the up-to-date versions.

<br>

### Script Access

<br>

#### Share the Scripts 

Whereas the `acc` container got us an accessible version of the data, the `salv` container is where we are going to do stuff with the data, which is facilitated by pre-written scripts and functions.

The scripts within the `acc` container include some functions, especially for reading the `.csv` files into `R`, that we would like to have within the `salv` container, so we follow suit for copying scripts:

```{bash, eval = FALSE}
docker cp acc:/scripts .
docker cp scripts salv:/
```

Note that this means the files in the `scripts` folder will overwrite any already-existing files in the `salv` container.
This also means that a user can add more scripts of their choosing into the container by having them in the local `scripts` folder prior to running the above line.

<br>

### Site Access

<br>

#### Bring the Site into `salv`

The website is comprised of a suite a folders and files that are contained within a folder named `site`.
The `accessor` image has nothing to do with the site whatsoever, so we do not need to copy anything from the container, but we do have a local instance of the `site` folder, which we copy into `salv`:


```{bash, eval = FALSE}
docker cp site salv:/
```

<br>

## Salvage Analyses & Presentation

Having created a runtime environment within the `salv` container that includes the necessary programs, data, and scripts to analyze and present the salvage data, we can use the `docker exec` to execute a command within the container.
To keep the overall code tidy at the `travis.yml` level, we package all of the necessary R code for analyses and presentation, including website rendering, within a single script `salvage_script.R`, 
which we execute:

```{bash, eval = FALSE}
docker exec -i salv R -e "source('scripts/salvage_script.R')"
```

The `-i` flag keeps the container's `stdin` open.
The `-e` flag allows the input for the `R` command to be an expression. 

<br>

### `R` Salvage Script

The `salvage_script.R` file consists of a few component blocks of code, which leverage functions written in the scripts to orchestrate the analysis and presentation.
Presently, these include

1. Source the `acc` and `salv` functions
```{R, eval = FALSE}
source("scripts/r_functions.R")
source("scripts/salvage_functions.R")

```
2. Read the Database into `R`
```{R, eval = FALSE}v
salvage <- read_database()
``` 
  - `salvage` is a `list` of `data.frames` that is directly analagous to the `.accdb` database of tables and the folder of `.csv` files
  - `read_database` has arguments to allow direction to alternative file locations but default settings are for this pipeline
3. Calculate all daily volumes and counts for Chinook, Steelhead, Striped Bass, and Delta Smelt at both facilities:
```{R, eval = FALSE}
species <- c(1:3, 26)
daily_salvage_tab <- daily_salvage(salvage, "1993-01-01", Sys.Date(), species)
```
4. Calculate and plot the daily exported volumes from both facilities
```{R, eval = FALSE}
dates <- seq.Date(as.Date("2020-01-01"), Sys.Date(), 1)
exported_volumes <- daily_exported_volume(salvage, dates)
exported_volumes_fig(exported_volumes)

```
5. Render the website
```{R, eval = FALSE}
rmarkdown::render_site("site")
```
6. Notify
```{R, eval = FALSE}
message("completed")
```

<br>

### Adding to the `R` execution

It is possible to add to the executed `R` code in one of two ways: 
1. Add to the `salvage_script.R` file
2. Add to the `docker exec` command

For [1], add code directly to your local `scripts/salvage_script.R`, save the file, and re-run
```{bash, eval = FALSE}
docker cp scripts salv:/
``` 
to copy the file into the container.

For [2], if you are adding a brief bit of `R` code, you can simply expand the `R -e` character input
```{bash, eval = FALSE}
sudo docker exec -i salvage R -e "source('scripts/r_script.R'); <additional_R_code>"
```
(replacing <additional_R_code> with the actual code). 
Alternatively, if your code is complex enough to warrant its own script, save it to a file `scripts/<your_scripts_name>.R` (replacing <your_scripts_name> with the actual file name), copy the file into the container
```{bash, eval = FALSE}
docker cp scripts salv:/
``` 
and then expand the `R -e` character input
```{bash, eval = FALSE}
docker exec -i salv R -e "source('scripts/salvage_script.R'); source('scripts/<your_scripts_name>.R')"
```
Note that we are still running the main `salvage_script.R` first here, which sources the function, loads the data, and puts the daily salvage table and current year's daily exported volumes into the working environment (as `daily_salvage_tab` and `exported_volumes`, respsectively).

<br>

## Retrieving the Results

We use the `cp` command again to move the resulting data (summary files are saved in `data/summaries`) and site files from the `salv` container out to the local space
```{bash, eval = FALSE}
docker cp salv:/data .
docker cp salv:/site .
```

<br>

# Data Preparation

Having brought the data into R as-is, we can now prepare them for summaries and analyses.
We use the functions included in the [`salvage_functions.R` R script](https://github.com/dapperstats/salvage/blob/master/scripts/salvage_functions.R), which is included within the [`salvage` docker image](https://hub.docker.com/r/dapperstats/salvage) that provides a stable runtime environment for the analyses and output generation (including website rendering).

<br>

## Daily Summaries

The data in the Sample table are at the highest frequency (sample level, generally every 2-hr) and for the present work we summarize the volumes and counts at the daily level within each of two buildings (CVP, SWP).
For a given sample, the amount of time (minutes) the facility was pumping, the amount of time (minutes) of the pumping that was sampled, and the primary flow (cubic feet per second) are recorded.
We convert the flow to cubic meters per minute and assume an average primary flow over the sample times to estimate sample-specific pumping and sampling volumes, which we then sum within days to produce daily totals in 1000 m$^3$.
For comparison, we also include the total daily exported volume reported in the Sample table, converted to 1000 m$^3$.
This value should be similar to the calculated daily totals, and indeed the values are very strongly aligned (correlation coefficient of 0.952 for CVP and 0.996 for SWP for 9,893 daily values from 1993-01-01 to 2020-02-01).
As an additional sample size metric, we also include the total number of included entries (samples) within each day for each building (general target being 12). 

To determine the total daily counts for each organism of interest, we gather the entries in the Catch table that are aligned with all samples for that day in that building and sum across their counts. 
These counts are the number of times a fish of that species was found in the processed sample volume, which is a fraction of the total volume sampled.
Thus, for a quick and simple expansion to a full-volume estimate, we divide the count by the sample volume and multiply by the pumped volume (as calculated).

<br>

# References 

White, E. P., G. M. Yenni, S. Taylor, E. Christensen, E. Bledsoe, J. L. Simonis, and S. K. M. Ernest. 2019. Developing an automated iterative near-term forecasting system for an ecological study. Methods in Ecology and Evolution 10:332-344. DOI: [10.1111/2041-210X.13104](https://doi.org/10.1111/2041-210x.13104).
